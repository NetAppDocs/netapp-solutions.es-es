---
sidebar: sidebar 
permalink: data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html 
keywords: AWS cloud, ha pair, high availability, openmessage benchmarking, architectural setup 
summary: Se realizó una prueba de rendimiento de un clúster Kafka con capa de almacenamiento montada en NFS de NetApp en el cloud de AWS. Los ejemplos de pruebas comparativas se describen en las siguientes secciones. 
---
= Información general y validación del rendimiento en AWS
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:kafka-nfs-why-netapp-nfs-for-kafka-workloads.html["Anterior: ¿Por qué NFS de NetApp para cargas de trabajo de Kafka?"]

[role="lead"]
Se realizó una prueba de rendimiento de un clúster Kafka con capa de almacenamiento montada en NFS de NetApp en el cloud de AWS. Los ejemplos de pruebas comparativas se describen en las siguientes secciones.



== Kafka en el cloud de AWS con Cloud Volumes ONTAP de NetApp (pareja de alta disponibilidad y nodo único)

Se realizó una prueba de rendimiento de un clúster de Kafka con Cloud Volumes ONTAP de NetApp (pareja de alta disponibilidad) en el cloud de AWS. Esta evaluación comparativa se describe en las siguientes secciones.



=== Configuración de la arquitectura

En la siguiente tabla se muestra la configuración del entorno de un clúster de Kafka con NAS.

|===
| Componente de plataforma | Configuración del entorno 


| Kafka 3.2.3  a| 
* 3 zookeepers – t2.pequeño
* 3 servidores de broker: i3en.2xlarge
* 1 x Grafana – c5n.2xgrande
* 4 x productor/consumidor -- c5n.2xgrande *




| Sistema operativo en todos los nodos | RHEL8.6 


| Instancia de Cloud Volumes ONTAP de NetApp | Instancia de par DE ALTA DISPONIBILIDAD – m5dn.12xLarge x 2 node Single Node Instance - m5dn.12xLarge x 1 node 
|===


=== Configuración de ONTAP para volúmenes del clúster de NetApp

. Para el par de alta disponibilidad de Cloud Volumes ONTAP, hemos creado dos agregados con tres volúmenes en cada agregado de cada controladora de almacenamiento. Para el nodo único de Cloud Volumes ONTAP, creamos seis volúmenes en un agregado.
+
image:kafka-nfs-image25.png["Esta imagen muestra las propiedades de aggr3 y aggr22."]

+
image:kafka-nfs-image26.png["Esta imagen muestra las propiedades de aggr2."]

. Para mejorar el rendimiento de red, hemos habilitado redes de alta velocidad para el par de alta disponibilidad y el nodo único.
+
image:kafka-nfs-image27.png["Esta imagen muestra cómo activar redes de alta velocidad."]

. Hemos observado que el ONTAP NVRAM tenía más IOPS, por lo que hemos cambiado el IOPS a 2350 para el volumen raíz de Cloud Volumes ONTAP. El disco del volumen raíz en Cloud Volumes ONTAP tenía un tamaño de 47 GB. El siguiente comando ONTAP es para el par de alta disponibilidad, y el mismo paso es aplicable para el único nodo.
+
....
statistics start -object vnvram -instance vnvram -counter backing_store_iops -sample-id sample_555
kafka_nfs_cvo_ha1::*> statistics show -sample-id sample_555
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-01
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1479
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-02
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1210
2 entries were displayed.
kafka_nfs_cvo_ha1::*>
....
+
image:kafka-nfs-image28.png["Estas imágenes muestran la forma de modificar las propiedades del volumen."]



En la figura siguiente se muestra la arquitectura de un clúster Kafka basado en NAS.

* *Compute.* utilizamos un clúster Kafka de tres nodos con un conjunto de zoomkeeper de tres nodos que se ejecuta en servidores dedicados. Cada agente tenía dos puntos de montaje NFS en un único volumen de la instancia de Cloud Volumes ONTAP a través de un LIF dedicado.
* *Supervisión.* utilizamos dos nodos para una combinación Prometheus-Grafana. Para generar cargas de trabajo, utilizamos un clúster de tres nodos independiente que podía producir y consumir este clúster Kafka.
* *Almacenamiento.* utilizamos una instancia Cloud Volumes ONTAP de par de alta disponibilidad con un volumen GP3 de 6 TB AWS-EBS montado en la instancia. El volumen se exportó después al agente de Kafka con un montaje NFS.


image:kafka-nfs-image29.png["En esta figura, se muestra la arquitectura de un clúster Kafka basado en NAS."]



=== Configuraciones de pruebas de rendimiento de OpenMessage

. Para obtener un mejor rendimiento NFS, se necesitan más conexiones de red entre el servidor NFS y el cliente NFS, que se pueden crear utilizando nconnect. Monte los volúmenes NFS en los nodos de broker con la opción nconnect ejecutando el siguiente comando:
+
....
[root@ip-172-30-0-121 ~]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38/xfsdefaults00
/dev/nvme1n1 /mnt/data-1 xfs defaults,noatime,nodiscard 0 0
/dev/nvme2n1 /mnt/data-2 xfs defaults,noatime,nodiscard 0 0
172.30.0.233:/kafka_aggr3_vol1 /kafka_aggr3_vol1 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol2 /kafka_aggr3_vol2 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol3 /kafka_aggr3_vol3 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol1 /kafka_aggr22_vol1 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol2 /kafka_aggr22_vol2 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol3 /kafka_aggr22_vol3 nfs defaults,nconnect=16 0 0
[root@ip-172-30-0-121 ~]# mount -a
[root@ip-172-30-0-121 ~]# df -h
Filesystem                       Size  Used Avail Use% Mounted on
devtmpfs                          31G     0   31G   0% /dev
tmpfs                             31G  249M   31G   1% /run
tmpfs                             31G     0   31G   0% /sys/fs/cgroup
/dev/nvme0n1p2                    10G  2.8G  7.2G  28% /
/dev/nvme1n1                     2.3T  248G  2.1T  11% /mnt/data-1
/dev/nvme2n1                     2.3T  245G  2.1T  11% /mnt/data-2
172.30.0.233:/kafka_aggr3_vol1   1.0T   12G 1013G   2% /kafka_aggr3_vol1
172.30.0.233:/kafka_aggr3_vol2   1.0T  5.5G 1019G   1% /kafka_aggr3_vol2
172.30.0.233:/kafka_aggr3_vol3   1.0T  8.9G 1016G   1% /kafka_aggr3_vol3
172.30.0.242:/kafka_aggr22_vol1  1.0T  7.3G 1017G   1% /kafka_aggr22_vol1
172.30.0.242:/kafka_aggr22_vol2  1.0T  6.9G 1018G   1% /kafka_aggr22_vol2
172.30.0.242:/kafka_aggr22_vol3  1.0T  5.9G 1019G   1% /kafka_aggr22_vol3
tmpfs                            6.2G     0  6.2G   0% /run/user/1000
[root@ip-172-30-0-121 ~]#
....
. Compruebe las conexiones de red en Cloud Volumes ONTAP. El siguiente comando ONTAP se usa desde el nodo único de Cloud Volumes ONTAP. El mismo paso es aplicable al par de alta disponibilidad de Cloud Volumes ONTAP.
+
....
Last login time: 1/20/2023 00:16:29
kafka_nfs_cvo_sn::> network connections active show -service nfs* -fields remote-host
node                cid        vserver              remote-host
------------------- ---------- -------------------- ------------
kafka_nfs_cvo_sn-01 2315762628 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762629 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762630 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762631 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762632 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762633 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762634 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762635 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762636 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762637 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762639 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762640 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762641 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762642 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762643 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762644 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762645 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762646 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762647 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762648 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762649 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762650 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762651 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762652 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762653 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762656 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762657 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762658 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762659 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762660 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762661 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762662 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762663 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762664 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762665 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762666 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762667 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762668 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762669 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762670 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762671 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762672 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762673 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762674 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762676 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762677 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762678 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762679 svm_kafka_nfs_cvo_sn 172.30.0.223
48 entries were displayed.
 
kafka_nfs_cvo_sn::>
....
. Utilizamos el siguiente Kafka `server.properties` En todos los agentes de Kafka para el par de alta disponibilidad de Cloud Volumes ONTAP. La `log.dirs` la propiedad es diferente para cada agente, y las propiedades restantes son comunes para los corredores. Para corredura1, el `log.dirs` el valor es el siguiente:
+
....
[root@ip-172-30-0-121 ~]# cat /opt/kafka/config/server.properties
broker.id=0
advertised.listeners=PLAINTEXT://172.30.0.121:9092
#log.dirs=/mnt/data-1/d1,/mnt/data-1/d2,/mnt/data-1/d3,/mnt/data-2/d1,/mnt/data-2/d2,/mnt/data-2/d3
log.dirs=/kafka_aggr3_vol1/broker1,/kafka_aggr3_vol2/broker1,/kafka_aggr3_vol3/broker1,/kafka_aggr22_vol1/broker1,/kafka_aggr22_vol2/broker1,/kafka_aggr22_vol3/broker1
zookeeper.connect=172.30.0.12:2181,172.30.0.30:2181,172.30.0.178:2181
num.network.threads=64
num.io.threads=64
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
replica.fetch.max.bytes=524288000
background.threads=20
num.replica.alter.log.dirs.threads=40
num.replica.fetchers=20
[root@ip-172-30-0-121 ~]#
....
+
** Para corredura2, la `log.dirs` el valor de la propiedad es el siguiente:
+
....
log.dirs=/kafka_aggr3_vol1/broker2,/kafka_aggr3_vol2/broker2,/kafka_aggr3_vol3/broker2,/kafka_aggr22_vol1/broker2,/kafka_aggr22_vol2/broker2,/kafka_aggr22_vol3/broker2
....
** Para corredura3, el `log.dirs` el valor de la propiedad es el siguiente:
+
....
log.dirs=/kafka_aggr3_vol1/broker3,/kafka_aggr3_vol2/broker3,/kafka_aggr3_vol3/broker3,/kafka_aggr22_vol1/broker3,/kafka_aggr22_vol2/broker3,/kafka_aggr22_vol3/broker3
....


. Para el nodo único de Cloud Volumes ONTAP, el Kafka `servers.properties` Es lo mismo que para el par de alta disponibilidad de Cloud Volumes ONTAP, a excepción de la `log.dirs` propiedad.
+
** Para corredura1, el `log.dirs` el valor es el siguiente:
+
....
log.dirs=/kafka_aggr2_vol1/broker1,/kafka_aggr2_vol2/broker1,/kafka_aggr2_vol3/broker1,/kafka_aggr2_vol4/broker1,/kafka_aggr2_vol5/broker1,/kafka_aggr2_vol6/broker1
....
** Para corredura2, la `log.dirs` el valor es el siguiente:
+
....
log.dirs=/kafka_aggr2_vol1/broker2,/kafka_aggr2_vol2/broker2,/kafka_aggr2_vol3/broker2,/kafka_aggr2_vol4/broker2,/kafka_aggr2_vol5/broker2,/kafka_aggr2_vol6/broker2
....
** Para corredura3, el `log.dirs` el valor de la propiedad es el siguiente:
+
....
log.dirs=/kafka_aggr2_vol1/broker3,/kafka_aggr2_vol2/broker3,/kafka_aggr2_vol3/broker3,/kafka_aggr2_vol4/broker3,/kafka_aggr2_vol5/broker3,/kafka_aggr2_vol6/broker3
....


. La carga de trabajo en el OMB se configura con las siguientes propiedades: `(/opt/benchmark/workloads/1-topic-100-partitions-1kb.yaml)`.
+
....
topics: 4
partitionsPerTopic: 100
messageSize: 32768
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 100
subscriptionsPerTopic: 1
consumerPerSubscription: 80
producersPerTopic: 40
producerRate: 1000000
consumerBacklogSizeGB: 0
testDurationMinutes: 5
....
+
La `messageSize` puede variar para cada caso de uso. En nuestra prueba de rendimiento, utilizamos 3K.

+
Utilizamos dos controladores distintos, Sync o Throughput, de OMB, para generar la carga de trabajo en el clúster Kafka.

+
** El archivo yaml utilizado para las propiedades del controlador Sync es el siguiente `(/opt/benchmark/driver- kafka/kafka-sync.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
  flush.messages=1
  flush.ms=0
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....
** El archivo yaml utilizado para las propiedades del controlador de rendimiento es el siguiente `(/opt/benchmark/driver- kafka/kafka-throughput.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
  default.api.timeout.ms=1200000
  request.timeout.ms=1200000
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....






== Metodología de las pruebas

. Se aprovisionó un clúster de Kafka según las especificaciones descritas anteriormente con Terraform y Ansible. Terraform se utiliza para crear la infraestructura con instancias de AWS para el clúster de Kafka y Ansible crea el clúster de Kafka.
. Se activó una carga de trabajo de OMB con la configuración de carga de trabajo descrita anteriormente y el controlador de sincronización.
+
....
Sudo bin/benchmark –drivers driver-kafka/kafka- sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. Se activó otra carga de trabajo con el controlador de rendimiento con la misma configuración de carga de trabajo.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




== Observación

Se utilizaron dos tipos distintos de controladores para generar cargas de trabajo con el fin de llevar a cabo una prueba de rendimiento de una instancia de Kafka que se ejecuta en NFS. La diferencia entre los controladores es la propiedad log flush.

En un par de alta disponibilidad de Cloud Volumes ONTAP:

* Rendimiento total generado de forma coherente por el controlador Sync: ~1236 Mbps.
* Rendimiento total generado para el controlador de rendimiento: Pico de ~1412 Mbps.


Para un único nodo Cloud Volumes ONTAP:

* Rendimiento total generado de forma consistente por el controlador Sync: ~ 1962MBps.
* Rendimiento total generado por el controlador de rendimiento: Pico de ~1660 MB


El controlador Sync puede generar un rendimiento constante a medida que los registros se vacíen en el disco al instante, mientras que el controlador de rendimiento genera ráfagas de rendimiento a medida que los registros se envían al disco de forma masiva.

Estos números de rendimiento se generan para la configuración de AWS determinada. Para requisitos de rendimiento más altos, los tipos de instancias se pueden escalar verticalmente para mejorar los números de rendimiento. El rendimiento total o la tasa total es la combinación de la tasa de producción y del consumidor.

image:kafka-nfs-image30.png["Aquí se presentan cuatro gráficos diferentes. Controlador de rendimiento de par CVO-ha. Controlador de sincronización de pares CVO-ha. Controlador de rendimiento de nodo único CVO. Controlador CVO-Single Node Sync."]

Asegúrese de comprobar el rendimiento del almacenamiento al realizar el rendimiento o sincronizar las pruebas de rendimiento del controlador.

image:kafka-nfs-image31.png["Este gráfico muestra el rendimiento en la latencia, IOPS y rendimiento."]



== Apache Kafka en AWS FSxN



=== Descripción general

Network File System (NFS) es un sistema de archivos de red ampliamente utilizado para almacenar grandes cantidades de datos. En la mayoría de las organizaciones, los datos se generan cada vez más a partir de aplicaciones de streaming como Apache Kafka. Estas cargas de trabajo requieren escalabilidad, baja latencia y una arquitectura de ingesta de datos sólida con funcionalidades de almacenamiento modernas. Para permitir el análisis en tiempo real y proporcionar información procesable, es necesaria una infraestructura bien diseñada y de alto rendimiento.

Kafka by design funciona con un sistema de archivos compatible con POSIX y se basa en el sistema de archivos para gestionar las operaciones de archivos, pero al almacenar datos en un sistema de archivos NFSv3, el cliente NFS de Kafka Broker puede interpretar las operaciones de archivos de forma diferente a un sistema de archivos local como XFS o Ext4. Un ejemplo común es el nombre de NFS Silly, que causó que los agentes de Kafka fallaran al expandir clústeres y volver a asignar particiones. Para hacer frente a este reto, NetApp ha actualizado el cliente NFS de Linux de código abierto con cambios que ahora están disponibles de forma general en RHEL8,7, RHEL9,1, y son compatibles con el lanzamiento actual de FSx para ONTAP, ONTAP 9.12.1.

Amazon FSx para NetApp ONTAP proporciona un sistema de archivos NFS en la nube totalmente gestionado, escalable y de alto rendimiento. Los datos de Kafka en FSx para NetApp pueden escalarse para manejar grandes cantidades de datos y garantizar la tolerancia a fallos. NFS proporciona una gestión de almacenamiento centralizada y protección de datos para conjuntos de datos críticos y confidenciales.

Estas mejoras hacen posible que los clientes de AWS aprovechen FSx para ONTAP cuando ejecuten cargas de trabajo de Kafka en servicios de computación de AWS. Estos beneficios son:
* Reducir el uso de la CPU para reducir el tiempo de espera de E/S.
* Tiempo de recuperación de Kafka broker más rápido
* Fiabilidad y eficiencia
* Escalabilidad y rendimiento
* Disponibilidad de la zona de multidisponibilidad
* Protección de datos



=== Información general sobre el rendimiento y validación en AWS FSxN

En la FSxN de AWS se realizó una prueba de rendimiento de un clúster Kafka con la capa de almacenamiento montada en NFS de NetApp. Los ejemplos de pruebas comparativas se describen en las siguientes secciones.



==== Kafka en AWS FSxN (Active Passive)

Se realizó una prueba de rendimiento de un clúster Kafka con AWS FSxN en la nube de AWS. Esta evaluación comparativa se describe en las siguientes secciones.



==== Configuración de la arquitectura

La siguiente tabla muestra la configuración ambiental para un cluster Kafka usando AWS FSxN.

|===
| Componente de plataforma | Configuración del entorno 


| Kafka 3.2.3  a| 
* 3 zookeepers – t2.pequeño
* 3 servidores de broker: i3en.2xlarge
* 1 x Grafana – c5n.2xgrande
* 4 x productor/consumidor -- c5n.2xgrande *




| Sistema operativo en todos los nodos | RHEL8.6 


| FSxN de AWS | Instancia activa pasiva con un rendimiento de 4GB MB/s y 160000 IPS 
|===


==== Configuración de FSxN de NetApp

. Para nuestras pruebas iniciales, hemos creado un sistema de archivos FSx for NetApp ONTAP con 2TB GB y 40000 000 IOPS para un rendimiento de 2GB GB/s.
. En FSx para NetApp ONTAP, el número máximo de iops que se puede alcanzar para un sistema de archivos con un rendimiento de 2GB gb/s en nuestra región de prueba (US-East-1) es 80.000 000 iops. El número máximo total de iops para un sistema de archivos FSx para NetApp ONTAP es de 160.000 000 iops que requiere una puesta en marcha de 4GB GB/s de rendimiento para lograrlo, lo que demostraremos más adelante en este documento
+
....
[root@ip-172-31-33-69 ~]# aws fsx create-file-system --region us-east-2  --storage-capacity 2048 --subnet-ids <desired subnet 1> subnet-<desired subnet 2> --file-system-type ONTAP --ontap-configuration DeploymentType=MULTI_AZ_HA_1,ThroughputCapacity=2048,PreferredSubnetId=<desired primary subnet>,FsxAdminPassword=<new password>,DiskIopsConfiguration="{Mode=USER_PROVISIONED,Iops=40000"}
....
+
La sintaxis detallada de la línea de comandos para FSX “create-file-system” se puede encontrar aquí: https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html[]
Por ejemplo, puede especificar una clave KMS específica en lugar de la clave maestra de FSX predeterminada que se utiliza cuando no se especifica ninguna clave KMS.

. Espere hasta que el estado de “ciclo de vida” cambie a “DISPONIBLE” en su retorno JSON después de describir su sistema de archivos de la siguiente manera:
+
....
[root@ip-172-31-33-69 ~]# aws fsx describe-file-systems  --region us-east-1 --file-system-ids fs-02ff04bab5ce01c7c
....
. La contraseña para fsxadmin es la contraseña configurada cuando se crea el sistema de archivos por primera vez.
. Valide las credenciales iniciando sesión en FsxN mediante fsxadmin
+
....
[root@ip-172-31-33-69 ~]# ssh fsxadmin@198.19.250.244
The authenticity of host '198.19.250.244 (198.19.250.244)' can't be established.
ED25519 key fingerprint is SHA256:mgCyRXJfWRc2d/jOjFbMBsUcYOWjxoIky0ltHvVDL/Y.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '198.19.250.244' (ED25519) to the list of known hosts.
(fsxadmin@198.19.250.244) Password:

This is your first recorded login.
....
. Cree la máquina virtual de almacenamiento en FSxN
+
....
[root@ip-172-31-33-69 ~]# aws fsx --region us-east-1 create-storage-virtual-machine --name svmkafkatest --file-system-id fs-02ff04bab5ce01c7c
....
. SSH en el sistema de archivos recientemente creado FSX para NetApp ONTAP y crear volúmenes en la máquina virtual de almacenamiento usando el comando de ejemplo debajo y, de manera similar, creamos volúmenes 6 para esta validación. Según nuestra validación, mantener el componente predeterminado (8) o menos constituyentes proporciona un mejor rendimiento a kafka.
+
....
FsxId02ff04bab5ce01c7c::*> volume create -volume kafkafsxN1 -state online -policy default -unix-permissions ---rwxr-xr-x -junction-active true -type RW -snapshot-policy none  -junction-path /kafkafsxN1 -aggr-list aggr1
....
. Amplíe el tamaño del volumen a 2TB TB y monte en la ruta de unión.
+
....
FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN1 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN1" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN2 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN2" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN3 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN3" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN4 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN4" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN5 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN5" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN6 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN6" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume show -vserver svmkafkatest -volume *
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
svmkafkatest
          kafkafsxN1   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN2   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN3   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN4   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN5   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN6   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          svmkafkatest_root
                       aggr1        online     RW          1GB    968.1MB    0%
7 entries were displayed.

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN1 -junction-path /kafkafsxN1

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN2 -junction-path /kafkafsxN2

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN3 -junction-path /kafkafsxN3

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN4 -junction-path /kafkafsxN4

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN5 -junction-path /kafkafsxN5

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN6 -junction-path /kafkafsxN6
....
. Ampliamos la capacidad de rendimiento de FSxN de 2GB GB/s a 4GB GB/s e IOPS a 160000 GB
+
....
[root@ip-172-31-33-69 ~]# aws fsx update-file-system --region us-east-1  --storage-capacity 5120 --ontap-configuration 'ThroughputCapacity=4096,DiskIopsConfiguration={Mode=USER_PROVISIONED,Iops=160000}' --file-system-id fs-02ff04bab5ce01c7c
....
+
La sintaxis detallada de la línea de comandos para FSX “update-file-system” se puede encontrar aquí:
https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html[]

. Los volúmenes FSxN se montan con Nconnect y opiones predeterminados en los brókeres kafkar
+
image:aws-fsx-kafka-arch1.png["Esta imagen muestra la arquitectura de un clúster Kafka basado en FSxN."]

+
** Informática. Utilizamos un clúster Kafka de tres nodos con un conjunto de zookeeper de tres nodos ejecutándose en servidores dedicados. Cada corredor tenía seis puntos de montaje NFS a seis volúmenes en la instancia de FSxN.
** Supervisión. Utilizamos dos nodos para una combinación de Prometheus-Grafana. Para generar cargas de trabajo, utilizamos un clúster de tres nodos independiente que podía producir y consumir este clúster Kafka.
** Reducida. Utilizamos una FSxN con seis volúmenes de 1TB montados. El volumen se exportó después al agente de Kafka con un montaje NFS.






==== Configuraciones de OpenMessage Benchmarking.

Utilizamos la misma configuración utilizada para la ONTAP para volúmenes de cloud de NetApp. Encontrará sus detalles aquí -
https://docs.netapp.com/us-en/netapp-solutions/data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html#architectural-setup[]



==== Metodología de las pruebas

. Un clúster de Kafka fue aprovisionado según la especificación descrita anteriormente usando terraform y ansible. Terraform se utiliza para construir la infraestructura utilizando instancias de AWS para el clúster Kafka y ansible construye el clúster Kafka en ellos.
. Se activó una carga de trabajo de OMB con la configuración de carga de trabajo descrita anteriormente y el controlador de sincronización.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. Se activó otra carga de trabajo con el controlador de rendimiento con la misma configuración de carga de trabajo.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




==== Observación

Se utilizaron dos tipos distintos de controladores para generar cargas de trabajo con el fin de llevar a cabo una prueba de rendimiento de una instancia de Kafka que se ejecuta en NFS. La diferencia entre los controladores es la propiedad log flush.

Para un factor de replicación kafka 1 y el FSxN:

* Rendimiento total generado consistentemente por el controlador de sincronización: ~ 3218 Mbps y rendimiento máximo en ~ 3652 Mbps.
* Rendimiento total generado consistentemente por el controlador de rendimiento: ~ 3679 Mbps y rendimiento máximo en ~ 3908 Mbps.


Para kafka con factor de replicación 3 y FSxN :

* Rendimiento total generado consistentemente por el controlador de sincronización: ~ 1252 Mbps y rendimiento máximo en ~ 1382 Mbps.
* Rendimiento total generado consistentemente por el controlador de rendimiento: ~ 1218 Mbps y rendimiento máximo en ~ 1328 Mbps.


En el factor de replicación Kafka 3, la operación de lectura y escritura se produjo tres veces en el FSxN, en el factor de replicación Kafka 1, la operación de lectura y escritura es una vez en el FSxN, por lo que en ambas validaciones, podemos alcanzar el rendimiento máximo de 4GB MB/seg.

El controlador Sync puede generar un rendimiento constante a medida que los registros se vacíen en el disco al instante, mientras que el controlador de rendimiento genera ráfagas de rendimiento a medida que los registros se envían al disco de forma masiva.

Estos números de rendimiento se generan para la configuración de AWS determinada. Para requisitos de rendimiento más altos, los tipos de instancias se pueden escalar verticalmente para mejorar los números de rendimiento. El rendimiento total o la tasa total es la combinación de la tasa de producción y del consumidor.

image:aws-fsxn-performance-rf-1-rf-3.png["Esta imagen muestra el rendimiento de kafka con RF1 y RF3"]

En el siguiente gráfico se muestra el rendimiento FSxn de 2GB MB/s y 4GB MB/s para el factor de replicación kafka 3. El factor de replicación 3 realiza la operación de lectura y escritura tres veces en el almacenamiento FSxN. La tasa total para el controlador de rendimiento es de 881 MB/s, que lee y escribe la operación kafka aproximadamente 2,64 GB/s en el sistema de archivos FSxN de 2GB MB/s y la tasa total para el controlador de rendimiento es de 1328 MB/s que lee y escribe la operación kafka aproximadamente 3,98 GB/s. El rendimiento de Kafka es lineal y escalable en función del rendimiento FSxN.

image:aws-fsxn-2gb-4gb-scale.png["Esta imagen muestra el rendimiento de escalado horizontal de 2GB Gb/s y 4GB Gb/s."]

El siguiente gráfico muestra el rendimiento entre la instancia EC2 y FSxN (factor de replicación Kafka: 3)

image:aws-fsxn-ec2-fsxn-comparition.png["Esta imagen muestra la comparación de rendimiento de EC2 frente a FSxN en RF3."]

link:kafka-nfs-performance-overview-and-validation-with-aff-on-premises.html["Siguiente: Información general y validación del rendimiento con AFF en las instalaciones."]

---
sidebar: sidebar 
permalink: data-analytics/apache-spark-major-ai-ml-and-dl-use-cases-and-architectures.html 
keywords: nlp pipelines, tensorflow distributed inferenceing, horovod distributed training, multi-worker, deep learning, keras, ctr prediction 
summary: Esta página describe los principales casos prácticos de IA, ML y DL y arquitecturas en mayor detalle. 
---
= Principales casos de uso y arquitecturas de IA, APRENDIZAJE AUTOMÁTICO y aprendizaje profundo
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Los principales casos de uso y metodología de IA, APRENDIZAJE AUTOMÁTICO y aprendizaje profundo pueden dividirse en las siguientes secciones:



== Canalizaciones NLP Spark y la inferencia distribuida TensorFlow

La siguiente lista contiene las bibliotecas NLP de código abierto más populares que han adoptado la comunidad de ciencias de datos bajo diferentes niveles de desarrollo:

* https://www.nltk.org/["Kit de herramientas de lenguaje natural (NLMK)"^]. El kit de herramientas completo para todas las técnicas de NLP. Se mantiene desde principios de los años 2000.
* https://textblob.readthedocs.io/en/dev/["TextBlob"^]. Una herramienta NLP de fácil uso API de Python construida sobre NLMK y Pattern.
* https://stanfordnlp.github.io/CoreNLP/["Stanford Core NLP"^]. Servicios y paquetes NLP en Java desarrollados por el Grupo NLP de Stanford.
* https://radimrehurek.com/gensim/["Gensim"^]. Modelado de Temas para humanos comenzó como una colección de scripts de Python para el proyecto de la Biblioteca Checa de Matemáticas digitales.
* https://spacy.io/["La piratería"^]. Flujos de trabajo NLP industrial integrales con Python y Cython con aceleración de GPU para transformadores.
* https://fasttext.cc/["Fasttext"^]. Una biblioteca gratuita, ligera y de código abierto de NLP para el aprendizaje de textos y clasificación de frases creada por el laboratorio DE investigación DE IA (FAIR) de Facebook.


Spark NLP es una solución única y unificada para todas las tareas y requisitos de NLP que permite un software escalable, de alto rendimiento y de alta precisión impulsado por NLP para casos de uso reales de producción. Aprovecha el aprendizaje de transferencia e implementa los últimos algoritmos y modelos de última generación en investigación y en diferentes industrias. Debido a la falta de pleno apoyo por parte de Spark para las bibliotecas anteriores, Spark NLP se creó sobre la base de https://spark.apache.org/docs/latest/ml-guide.html["Estimular ML"^] Aprovechar el motor de procesamiento de datos distribuidos en memoria de uso general de Spark como biblioteca de NLP de clase empresarial para flujos de trabajo de producción esenciales. Sus anotadores utilizan algoritmos basados en reglas, aprendizaje automático y TensorFlow para impulsar las implementaciones de aprendizaje profundo. Esto cubre las tareas comunes de las NLP, incluyendo, entre otras cosas, la tokenización, la limmatización, la secución, el marcado parcial del habla, el reconocimiento de entidades denominadas, revisión ortográfica y análisis de confianza.

Representaciones del codificador bidireccional de Transformers (BERT) es una técnica de aprendizaje automático basada en transformadores para NLP. Popularizó el concepto de preentrenamiento y ajuste fino. La arquitectura del transformador en BERT se originó a partir de la traducción automática, que modela las dependencias a largo plazo mejor que los modelos de lenguaje basados en Red neuronal recurrente (RNN). También introdujo la tarea Modelización de Lenguaje Mask (MLM), donde un 15% aleatorio de todos los tokens están enmascarados y el modelo los predice, permitiendo una verdadera bidirectionality.

El análisis de la confianza financiera es un reto debido al lenguaje especializado y a la falta de datos etiquetados en ese dominio. https://nlp.johnsnowlabs.com/2021/11/03/bert_sequence_classifier_finbert_en.html["FinBERT"^], Un modelo de idioma basado en BERT preformado, fue adaptado el dominio en https://trec.nist.gov/data/reuters/reuters.html["Reuters TRC2"^], un corpus financiero y ajustado con datos etiquetados ( https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10["Banco de frases financieras"^]) para la clasificación de la confianza financiera. Los investigadores extrajeron 4, 500 frases de artículos de noticias con términos financieros. Luego 16 expertos y maestros estudiantes con fondos financieros etiquetaron las frases como positivas, neutrales y negativas. Hemos creado un flujo de trabajo completo de Spark para analizar el sentimiento de las transcripciones de llamadas de beneficios de empresas del mejor de los 10 NASDAQ de 2016 a 2020 usando FinBERT y otros dos oleoductos preentrenados ( https://nlp.johnsnowlabs.com/2021/11/11/classifierdl_bertwiki_finance_sentiment_pipeline_en.html["Análisis de confianza para las noticias financieras"^], https://nlp.johnsnowlabs.com/2020/03/19/explain_document_dl.html["Explicar el documento DL"^]) Desde Spark NLP.

El motor de aprendizaje profundo subyacente para Spark NLP es TensorFlow, una plataforma integral de código abierto para el aprendizaje automático que permite una creación de modelos fácil, una producción de ML sólida en cualquier lugar y una experimentación potente para la investigación. Por lo tanto, al ejecutar nuestras tuberías en Spark `yarn cluster` Modo, estábamos ejecutando TensorFlow distribuido con datos y paralelización de modelos en un nodo maestro y varios de trabajadores, además de almacenamiento conectado a la red montado en el clúster.



== Horovod distribuyó entrenamiento

La validación principal de Hadoop para el rendimiento relacionado con MapReduce se realiza con TeraGen, TeraSort, TeraValidate y DFSIO (lectura y escritura). Los resultados de la validación TeraGen y TeraSort se presentan en http://www.netapp.com/us/media/tr-3969.pdf["TR-3969: Soluciones de NetApp para Hadoop"^] Para E-Series y en la sección “Storage Tiering” (xref) para AFF.

En función de las solicitudes de los clientes, consideramos que la formación distribuida con Spark es una de las más importantes de las distintas situaciones de uso. En este documento, utilizamos la https://horovod.readthedocs.io/en/stable/spark_include.html["Hovorod en Spark"^] Para validar el rendimiento de Spark con NetApp en las instalaciones, nativas del cloud e híbridas mediante las controladoras de almacenamiento All Flash FAS (AFF) de NetApp, Azure NetApp Files y StorageGRID.

El paquete Horovod en Spark proporciona un contenedor adecuado en torno a Horovod que simplifica la ejecución de cargas de trabajo de entrenamiento distribuidas en clústeres de Spark, lo cual permite crear un bucle de diseño de modelo ajustado en el que el procesamiento de datos, el entrenamiento de modelos y la evaluación de modelos se llevan a cabo en Spark donde se encuentran los datos de entrenamiento e inferencia.

Hay dos API para ejecutar Horovod en Spark: Una API de Estimator de alto nivel y una API de Run de bajo nivel. Aunque ambos usan el mismo mecanismo subyacente para lanzar Horovod en los ejecutores de Spark, la API de Estimator abstrae el procesamiento de datos, el bucle de entrenamiento del modelo, la verificación del modelo, la recopilación de métricas y la formación distribuida. Utilizamos Horovod Spark Estimators, TensorFlow y Keras para una preparación de datos completa y un flujo de trabajo de entrenamiento distribuido basado en la https://www.kaggle.com/c/rossmann-store-sales["Ventas de la tienda Kaggle Rossmann"^] competencia.

El script `keras_spark_horovod_rossmann_estimator.py` puede encontrarse en la sección link:apache-spark-python-scripts-for-each-major-use-case.html["Scripts Python para cada caso de uso principal."] Contiene tres partes:

* La primera parte realiza varios pasos de preprocesamiento de datos sobre un conjunto inicial de archivos CSV proporcionados por Kaggle y reunidos por la comunidad. Los datos de entrada se separan en un grupo de entrenamiento con un `Validation` un subconjunto y un conjunto de datos de pruebas.
* La segunda parte define un modelo de red neuronal profunda Keras (DNN) con función de activación sigmoide logarítmica y un optimizador Adam, y realiza un entrenamiento distribuido del modelo mediante Horovod en Spark.
* La tercera parte realiza predicciones en el conjunto de datos de pruebas utilizando el mejor modelo que minimiza el error absoluto de la media general del conjunto de validación. A continuación, crea un archivo CSV de salida.


Consulte la sección link:apache-spark-use-cases-summary.html#machine-learning["“Aprendizaje automático”"] para varios resultados de comparación de tiempo de ejecución.



== Aprendizaje profundo para múltiples trabajadores usando Keras para la predicción CTR

Con los recientes avances en las plataformas Y aplicaciones DE ML, ahora se presta mucha atención al aprendizaje a escala. La tasa de clics (CTR) se define como el número medio de clics-throughs por cien impresiones de anuncios en línea (expresado como porcentaje). Se ha adoptado ampliamente como métrica clave en diversos mercados verticales del sector y casos de uso, incluidos el marketing digital, el comercio minorista, el comercio electrónico y los proveedores de servicios. Consulte nuestra https://docs.netapp.com/us-en/netapp-solutions/ai/aks-anf_introduction.html["TR-4904: Formación distribuida en Azure - predicción de frecuencias mediante clic"^] Para obtener más información sobre las aplicaciones de CTR y una implementación de flujo de trabajo integral de Cloud AI con Kubernetes, ETL de datos distribuidos y entrenamiento de modelos con DASK y CUDA ML.

En este informe técnico se utilizó una variación del https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/["Criteo Terabyte haga clic en el conjunto de datos Logs"^] (Consulte TR-4904) para el aprendizaje profundo distribuido por varios trabajadores mediante Keras para crear un flujo de trabajo de Spark con modelos Deep and Cross Network (DCN), donde se compara su rendimiento en términos de una función de error de pérdida de registro con un modelo de regresión logística de Spark ML básico. DCN capta eficazmente las interacciones eficaces de funciones de grados limitados, aprende interacciones muy no lineales, no requiere ingeniería de funciones manuales ni búsqueda exhaustiva y tiene un bajo coste computacional.

Los datos para los sistemas de recomendación a escala web son en su mayoría discretos y categóricos, lo que conduce a un gran espacio de funciones escasas que resulta difícil para la exploración de funciones. Esto ha limitado la mayoría de los sistemas a gran escala a modelos lineales como la regresión logística. Sin embargo, la clave para hacer buenas predicciones es identificar las características predictivas más frecuentes y, al mismo tiempo, explorar características cruzadas poco visibles o raras. Los modelos lineales son simples, interpretables y fáciles de escalar, pero están limitados en su poder expresivo.

Por otro lado, se ha demostrado que las características cruzadas son significativas para mejorar la expresividad de los modelos. Desafortunadamente, a menudo requiere ingeniería de funciones manual o búsqueda exhaustiva para identificar estas funciones. Generalizar a interacciones de características no vistas es a menudo difícil. El uso de una red neuronal cruzada como DCN evita la ingeniería de funciones específicas de una tarea mediante la aplicación explícita de un cruce de funciones de forma automática. La red cruzada consta de varias capas, donde el grado más alto de interacciones se determina de forma comprobable por la profundidad de la capa. Cada capa produce interacciones de mayor orden basadas en las existentes y mantiene las interacciones de capas anteriores.

Una red neuronal profunda (DNN) tiene la promesa de capturar interacciones muy complejas entre características. Sin embargo, en comparación con DCN, requiere casi un orden de magnitud más parámetros, no es capaz de formar características cruzadas explícitamente y puede no aprender eficazmente algunos tipos de interacciones de características. La red cruzada es eficiente en la memoria y fácil de implementar. El entrenamiento conjunto de los componentes cross y DNN captura de forma eficiente las interacciones predictivas con las características y proporciona un rendimiento de última generación en el conjunto de datos Criteo CTR.

Un modelo DCN comienza con una capa de incrustación y apilado, seguido por una red transversal y una red profunda en paralelo. A su vez, están seguidas por una última capa de combinación que combina las salidas de las dos redes. Los datos de entrada pueden ser un vector con características dispersas y densas. En Spark, ambas https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.linalg.SparseVector.html["ml"^] y.. https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.mllib.linalg.SparseVector.html["mllib"^] las bibliotecas contienen el tipo `SparseVector`. Por lo tanto, es importante que los usuarios distingan entre los dos y sean conscientes cuando llaman a sus respectivas funciones y métodos. En sistemas de recomendación a escala web, como la predicción CTR, los insumos son, en su mayoría, características categóricas, por ejemplo `‘country=usa’`. Tales características se codifican a menudo como vectores de una caliente, por ejemplo, `‘[0,1,0, …]’`. Codificación en caliente (OHE) con `SparseVector` es útil cuando se trata de conjuntos de datos del mundo real con vocabularios en constante cambio y crecimiento. Hemos modificado ejemplos en https://github.com/shenweichen/DeepCTR["DeepCTR"^] Procesar grandes vocabularios, creando vectores de incrustación en la capa de incrustación y apilado de nuestro DCN.

La https://www.kaggle.com/competitions/criteo-display-ad-challenge/data["Conjunto de datos de anuncios de visualización Criteo"^] predice la velocidad de clic del ads. Tiene 13 características de enteros y 26 características categóricas en las cuales cada categoría tiene una cardinalidad alta. Para este conjunto de datos, una mejora de 0.001 en pérdida de registro es prácticamente significativa debido al gran tamaño de entrada. Una pequeña mejora en la precisión de la predicción para una gran base de usuarios puede conducir potencialmente a un gran aumento en los ingresos de una empresa. El conjunto de datos contiene 11 GB de registros de usuarios de un periodo de 7 días, lo que equivale a unos 41 millones de registros. Utilizamos Spark `dataFrame.randomSplit()function` dividir de forma aleatoria los datos para el entrenamiento (80%), la validación cruzada (10%) y el 10% restante para las pruebas.

DCN se implementó en TensorFlow con Keras. La aplicación del proceso de formación modelo con DCN consta de cuatro componentes principales:

* *Procesamiento e incrustación de datos.* las características con valor real se normalizan aplicando una transformación de registro. Para las características categóricas, incrustamos las características en vectores densos de la dimensión 6×(cardinalidad de categoría)1/4. La concatenación de todos los embeddings da como resultado un vector de dimensión 1026.
* * Optimización.* aplicamos la optimización estocástica de minilotes con el optimizador Adam. El tamaño de lote se estableció en 512. La normalización por lotes se aplicó a la red profunda y la norma de clip degradado se estableció en 100.
* *Regularización.* utilizamos la parada temprana, ya que la regularización o el abandono L2 no se encontró para ser eficaz.
* * Hiperparámetros.* reportamos resultados basados en una búsqueda de cuadrícula sobre el número de capas ocultas, el tamaño de la capa oculta, la tasa de aprendizaje inicial y el número de capas cruzadas. El número de capas ocultas oscilaba entre 2 y 5, con un tamaño de capa oculto que oscilaba entre 32 y 1024. Para DCN, el número de capas cruzadas fue de 1 a 6. La tasa de aprendizaje inicial se ajustó de 0.0001 a 0.001 con incrementos de 0.0001. Todos los experimentos se pararon temprano en la etapa de entrenamiento 150,000, más allá de la cual empezó a ocurrir el ajuste excesivo.


Además de DCN, también hemos probado otros modelos populares de aprendizaje profundo para la predicción CTR, como https://www.ijcai.org/proceedings/2017/0239.pdf["DeepFM"^], https://arxiv.org/pdf/1803.05170.pdf["XDeepFM"^], https://arxiv.org/abs/1810.11921["AutoInt"^], y. https://arxiv.org/abs/2008.13535["DCN v2"^].



== Arquitecturas que se utilizan para la validación

Para esta validación, hemos utilizado cuatro nodos de trabajo y uno de nodo maestro con un par de alta disponibilidad de AFF-A800. Todos los miembros del clúster se conectaron mediante switches de red de 10 GbE.

Para esta validación de la solución Spark de NetApp, se utilizaron tres controladoras de almacenamiento distintas: El E5760, el E5724 y AFF-A800. Las controladoras de almacenamiento E-Series se conectaron a cinco nodos de datos con conexiones SAS de 12 Gbps. La controladora de almacenamiento de la pareja de alta disponibilidad de AFF proporciona volúmenes NFS exportados a través de conexiones 10 GbE a los nodos de trabajo de Hadoop. Los miembros de los clústeres de Hadoop se conectaron mediante conexiones 10 GbE en las soluciones E-Series, AFF y Hadoop de StorageGRID.

image:apache-spark-image10.png["Arquitecturas que se utilizan para la validación."]

---
sidebar: sidebar 
permalink: ai/aipod_nv_architecture.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: 'NetApp AI Pod con sistemas NVIDIA DGX: Arquitectura' 
---
= NetApp AI Pod con sistemas NVIDIA DGX: Arquitectura
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_sw_components.html["Anterior: ONTAP AI - Componentes de software."]

Esta arquitectura de referencia aprovecha estructuras independientes para la interconexión en clústeres de computación y el acceso al almacenamiento, con opciones para la conectividad Infiniband (IB) de NDR200 e HDR200 Gb entre los nodos de computación. Los sistemas DGX H100 incluyen tarjetas ConnectX-7 preinstaladas para conectividad NDR IB, mientras que los sistemas DGX A100 pueden utilizar tarjetas ConnectX-6 o ConnectX-7 para conectividad HDR o NDR respectivamente.



== IA de ONTAP con sistemas DGX H100

El siguiente dibujo muestra la topología general de la solución cuando se utilizan sistemas DGX H100 con ONTAP AI.

image:oai_H100_topo.png["Error: Falta la imagen gráfica"]

En esta configuración, la red de clústeres de computación utiliza un par de QM9700 switches IB NDR, que se conectan entre sí para conseguir una alta disponibilidad. Cada sistema DGX H100 está conectado a los switches mediante ocho conexiones NDR200 GbE, con puertos numerados pares conectados a un switch y puertos con número impar conectados al otro switch.

Para el acceso al sistema de almacenamiento, la gestión en banda y el acceso de clientes, se utiliza un par de switches Ethernet SN4600 Gb. Los switches están conectados con enlaces entre switches y se configuran con varias VLAN para aislar los distintos tipos de tráfico. Para puestas en marcha mayores, la red ethernet se puede ampliar a una configuración hoja-espina añadiendo pares de switches adicionales para una espina y hojas adicionales según sea necesario. Cada sistema DGX A100 está aprovisionado con dos tarjetas ConnectX-6 de doble puerto para el tráfico ethernet y de almacenamiento. Para esta solución, los cuatro puertos están conectados a los switches Ethernet de SN4600 Gb a 200 Gbps. Un puerto de cada tarjeta se configura en un vínculo LACP MLAG con un puerto conectado a cada switch, y las VLAN para la gestión en banda, el acceso de clientes y el acceso al almacenamiento en el nivel de usuario se alojan en este vínculo. El otro puerto de cada tarjeta se utiliza de forma independiente en VLAN de almacenamiento RoCE dedicadas independientes para la conectividad con el sistema de almacenamiento AFF A800. Estos puertos admiten acceso a almacenamiento de alto rendimiento mediante NFS v3, NFSv4.x con pNFS y NFS sobre RDMA.

Además de la interconexión informática y las redes ethernet de alta velocidad, todos los dispositivos físicos también están conectados a uno o más switches Ethernet de SN2201 Gb para la gestión fuera de banda.  Para obtener más información sobre la conectividad del sistema DGX A100, consulte la link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentación de NVIDIA BasePOD"].



== IA de ONTAP con sistemas DGX A100

El siguiente dibujo muestra la topología general de la solución al utilizar sistemas DGX A100 y una estructura de computación HDR con ONTAP AI.

image:oai_A100_topo.png["Error: Falta la imagen gráfica"]

En esta configuración, la red de clúster de computación utiliza un par de switches IB HDR QM8700, que se conectan entre sí para lograr una alta disponibilidad. Cada sistema DGX A100 está conectado a los switches mediante cuatro tarjetas ConnectX-6 de un puerto a 200 Gbps, con puertos pares conectados a un switch y a puertos con números impares conectados al otro switch.

Para el acceso al sistema de almacenamiento, la gestión en banda y el acceso de clientes, se utiliza un par de switches Ethernet SN4600 Gb. Los switches están conectados con enlaces entre switches y se configuran con varias VLAN para aislar los distintos tipos de tráfico. Para puestas en marcha mayores, la red ethernet se puede ampliar a una configuración hoja-espina añadiendo pares de switches adicionales para una espina y hojas adicionales según sea necesario. Cada sistema DGX A100 está aprovisionado con dos tarjetas ConnectX-6 de doble puerto para el tráfico ethernet y de almacenamiento. Para esta solución, los cuatro puertos están conectados a los switches Ethernet de SN4600 Gb a 200 Gbps. Un puerto de cada tarjeta se configura en un vínculo LACP MLAG con un puerto conectado a cada switch, y las VLAN para la gestión en banda, el acceso de clientes y el acceso al almacenamiento en el nivel de usuario se alojan en este vínculo. El otro puerto de cada tarjeta se utiliza de forma independiente en VLAN de almacenamiento RoCE dedicadas independientes para la conectividad con el sistema de almacenamiento AFF A800. Estos puertos admiten acceso a almacenamiento de alto rendimiento mediante NFS v3, NFSv4.x con pNFS y NFS sobre RDMA.

Además de la interconexión informática y las redes ethernet de alta velocidad, todos los dispositivos físicos también están conectados a uno o más switches Ethernet de SN2201 Gb para la gestión fuera de banda.  Para obtener más información sobre la conectividad del sistema DGX A100, consulte la link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentación de NVIDIA BasePOD"].



== Servidores del plano de gestión

Esta arquitectura de referencia también incluye cinco servidores basados en CPU para los usos del plano de gestión. Dos de estos sistemas se utilizan como nodos principales de Base Command Manager para la puesta en marcha y gestión del clúster. Los otros tres sistemas se utilizan para proporcionar servicios de clúster adicionales, como los nodos maestros de Kubernetes o los nodos de inicio de sesión para las implementaciones que utilizan Slurm para la programación de tareas. Las puestas en marcha que utilizan Kubernetes pueden aprovechar el controlador CSI Astra Trident de NetApp para proporcionar aprovisionamiento automatizado y servicios de datos con almacenamiento persistente para cargas de trabajo de gestión y de IA en el sistema de almacenamiento A800 de AFF.

Cada servidor está conectado físicamente a los switches IB y ethernet para permitir la puesta en marcha y gestión de clústeres. Además, está configurado con montajes NFS en el sistema de almacenamiento a través de la SVM de gestión para almacenamiento de artefactos de gestión de clústeres, tal como se ha descrito anteriormente.

link:aipod_nv_storage.html["Próximo: NetApp AI Pod con sistemas NVIDIA DGX: Diseño del sistema de almacenamiento y guía de tamaño."]

---
sidebar: sidebar 
permalink: databases/cli_automation.html 
keywords: Linux, RHEL Oracle19c, NFS, ONTAP 
summary: Esta página describe el método automatizado para poner en marcha Oracle19c en el almacenamiento ONTAP de NetApp. 
---
= Procedimiento de puesta en marcha paso a paso
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
En este documento se detalla el despliegue de Oracle 19C mediante la interfaz de línea de comandos (cli) de automatización.



== Puesta en marcha de la interfaz de línea de comandos Oracle 19c Database

En esta sección se tratan los pasos necesarios para preparar y poner en marcha la base de datos Oracle19c con la CLI. Asegúrese de haber revisado el link:getting_started_requirements.html["Introducción y sección requisitos"] y preparar su entorno de acuerdo con sus necesidades.



=== Descargar Oracle19c repo

. Desde la controladora ansible, ejecute el siguiente comando:
+
[source, cli]
----
git clone https://github.com/NetApp-Automation/na_oracle19c_deploy.git
----
. Después de descargar el repositorio, cambie los directorios a na_oracle19c_Deploy <cd na_oracle19c_deploy>.




=== Edite el archivo hosts

Complete lo siguiente antes de la implementación:

. Edite el directorio na_oracle19c_deploy del archivo de host.
. En [ONTAP], cambie la dirección IP a la IP de administración del clúster.
. En el grupo [oracle], agregue los nombres de los hosts oracle. El nombre de host se debe resolver a su dirección IP a través de DNS o del archivo hosts, o bien debe especificarse en el host.
. Después de completar estos pasos, guarde los cambios.


En el ejemplo siguiente se muestra un archivo host:

[source, shell]
----
#ONTAP Host

[ontap]

"10.61.184.183"

#Oracle hosts

[oracle]

"rtpora01"

"rtpora02"
----
En este ejemplo se ejecuta el libro de aplicaciones y se implementa oracle 19c en dos servidores oracle DB simultáneamente. También puede realizar pruebas con un solo servidor de base de datos. En ese caso, sólo es necesario configurar un archivo de variable de host.


NOTE: El libro de estrategia se ejecuta de la misma manera independientemente de la cantidad de hosts y bases de datos de Oracle que se implementen.



=== Edite el archivo host_name.yml en host_var

Cada host de Oracle tiene su archivo de variable de host identificado por su nombre de host que contiene variables específicas del host. Es posible especificar cualquier nombre para el host. Edite y copie el `host_vars` Desde la sección Host VARS Config y péguela en su deseado `host_name.yml` archivo.


NOTE: Los elementos en azul deben cambiarse para que coincidan con su entorno.



=== Host VARS Config

[source, shell]
----
######################################################################
##############      Host Variables Configuration        ##############
######################################################################

# Add your Oracle Host
ansible_host: "10.61.180.15"

# Oracle db log archive mode: true - ARCHIVELOG or false - NOARCHIVELOG
log_archive_mode: "true"

# Number of pluggable databases per container instance identified by sid. Pdb_name specifies the prefix for container database naming in this case cdb2_pdb1, cdb2_pdb2, cdb2_pdb3
oracle_sid: "cdb2"
pdb_num: "3"
pdb_name: "{{ oracle_sid }}_pdb"

# CDB listener port, use different listener port for additional CDB on same host
listener_port: "1523"

# CDB is created with SGA at 75% of memory_limit, MB. Consider how many databases to be hosted on the node and how much ram to be allocated to each DB. The grand total SGA should not exceed 75% available RAM on node.
memory_limit: "5464"

# Set "em_configuration: DBEXPRESS" to install enterprise manager express and choose a unique port from 5500 to 5599 for each sid on the host.
# Leave them black if em express is not installed.
em_configuration: "DBEXPRESS"
em_express_port: "5501"

# {{groups.oracle[0]}} represents first Oracle DB server as defined in Oracle hosts group [oracle]. For concurrent multiple Oracle DB servers deployment, [0] will be incremented for each additional DB server. For example,  {{groups.oracle[1]}}" represents DB server 2, "{{groups.oracle[2]}}" represents DB server 3 ... As a good practice and the default, minimum three volumes is allocated to a DB server with corresponding /u01, /u02, /u03 mount points, which store oracle binary, oracle data, and oracle recovery files respectively. Additional volumes can be added by click on "More NFS volumes" but the number of volumes allocated to a DB server must match with what is defined in global vars file by volumes_nfs parameter, which dictates how many volumes are to be created for each DB server.
host_datastores_nfs:
  - {vol_name: "{{groups.oracle[0]}}_u01", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}
  - {vol_name: "{{groups.oracle[0]}}_u02", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}
  - {vol_name: "{{groups.oracle[0]}}_u03", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}
----


=== Edite el archivo var.yml

La `vars.yml` File consolida todas las variables específicas de un entorno (ONTAP, Linux u Oracle) para la puesta en marcha de Oracle.

. Edite y copie las variables de la sección VARS y pegue estas variables en su `vars.yml` archivo.


[source, shell]
----
#######################################################################
###### Oracle 19c deployment global user configuration variables ######
######  Consolidate all variables from ontap, linux and oracle   ######
#######################################################################

###########################################
### Ontap env specific config variables ###
###########################################

#Inventory group name
#Default inventory group name - 'ontap'
#Change only if you are changing the group name either in inventory/hosts file or in inventory groups in case of AWX/Tower
hosts_group: "ontap"

#CA_signed_certificates (ONLY CHANGE to 'true' IF YOU ARE USING CA SIGNED CERTIFICATES)
ca_signed_certs: "false"

#Names of the Nodes in the ONTAP Cluster
nodes:
 - "AFF-01"
 - "AFF-02"

#Storage VLANs
#Add additional rows for vlans as necessary
storage_vlans:
   - {vlan_id: "203", name: "infra_NFS", protocol: "NFS"}
More Storage VLANsEnter Storage VLANs details

#Details of the Data Aggregates that need to be created
#If Aggregate creation takes longer, subsequent tasks of creating volumes may fail.
#There should be enough disks already zeroed in the cluster, otherwise aggregate create will zero the disks and will take long time
data_aggregates:
  - {aggr_name: "aggr01_node01"}
  - {aggr_name: "aggr01_node02"}

#SVM name
svm_name: "ora_svm"

# SVM Management LIF Details
svm_mgmt_details:
  - {address: "172.21.91.100", netmask: "255.255.255.0", home_port: "e0M"}

# NFS storage parameters when data_protocol set to NFS. Volume named after Oracle hosts name identified by mount point as follow for oracle DB server 1. Each mount point dedicates to a particular Oracle files: u01 - Oracle binary, u02 - Oracle data, u03 - Oracle redo. Add additional volumes by click on "More NFS volumes" and also add the volumes list to corresponding host_vars as host_datastores_nfs variable. For multiple DB server deployment, additional volumes sets needs to be added for additional DB server. Input variable "{{groups.oracle[1]}}_u01", "{{groups.oracle[1]}}_u02", and "{{groups.oracle[1]}}_u03" as vol_name for second DB server. Place volumes for multiple DB servers alternatingly between controllers for balanced IO performance, e.g. DB server 1 on controller node1, DB server 2 on controller node2 etc. Make sure match lif address with controller node.

volumes_nfs:
  - {vol_name: "{{groups.oracle[0]}}_u01", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}
  - {vol_name: "{{groups.oracle[0]}}_u02", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}
  - {vol_name: "{{groups.oracle[0]}}_u03", aggr_name: "aggr01_node01", lif: "172.21.94.200", size: "25"}

#NFS LIFs IP address and netmask

nfs_lifs_details:
  - address: "172.21.94.200" #for node-1
    netmask: "255.255.255.0"
  - address: "172.21.94.201" #for node-2
    netmask: "255.255.255.0"

#NFS client match

client_match: "172.21.94.0/24"

###########################################
### Linux env specific config variables ###
###########################################

#NFS Mount points for Oracle DB volumes

mount_points:
  - "/u01"
  - "/u02"
  - "/u03"

# Up to 75% of node memory size divided by 2mb. Consider how many databases to be hosted on the node and how much ram to be allocated to each DB.
# Leave it blank if hugepage is not configured on the host.

hugepages_nr: "1234"

# RedHat subscription username and password

redhat_sub_username: "xxx"
redhat_sub_password: "xxx"

####################################################
### DB env specific install and config variables ###
####################################################

db_domain: "your.domain.com"

# Set initial password for all required Oracle passwords. Change them after installation.

initial_pwd_all: "netapp123"
----


=== Ejecute el libro de estrategia

Después de completar los requisitos previos de entorno necesarios y copiar las variables en `vars.yml` y.. `your_host.yml`, ya está listo para implementar los libros de estrategia.


NOTE: debe cambiarse <username> para adecuarse a su entorno.

. Ejecute el libro de estrategia de ONTAP transfiriendo las etiquetas correctas y el nombre de usuario del clúster de ONTAP. Rellene la contraseña del clúster de ONTAP y vsadmin cuando se le solicite.
+
[source, cli]
----
ansible-playbook -i hosts all_playbook.yml -u username -k -K -t ontap_config -e @vars/vars.yml
----
. Ejecute Linux playbook para ejecutar la parte de la implementación de Linux. Entrada para la contraseña ssh de administrador así como la contraseña sudo.
+
[source, cli]
----
ansible-playbook -i hosts all_playbook.yml -u username -k -K -t linux_config -e @vars/vars.yml
----
. Ejecute Oracle playbook para ejecutar la parte del despliegue de Oracle. Entrada para la contraseña ssh de administrador así como la contraseña sudo.
+
[source, cli]
----
ansible-playbook -i hosts all_playbook.yml -u username -k -K -t oracle_config -e @vars/vars.yml
----




=== Ponga en marcha una base de datos adicional en el mismo host de Oracle

La parte Oracle del playbook crea una única base de datos de contenedor Oracle en un servidor Oracle por ejecución. Para crear una base de datos de contenedores adicional en el mismo servidor, lleve a cabo los siguientes pasos:

. Revise las variables host_var.
+
.. Vuelva al paso 3 - edite el `host_name.yml` archivo debajo `host_vars`.
.. Cambie el SID de Oracle a una cadena de nomenclatura diferente.
.. Cambie el puerto de escucha a un número diferente.
.. Si ha instalado EM Express, cambie el puerto de EM Express a otro número.
.. Copie y pegue las variables de host revisadas en el archivo de variable de host Oracle en `host_vars`.


. Ejecute el libro de estrategia con `oracle_config` etiquetar como se muestra arriba en la <<Ejecute el libro de estrategia>>.




=== Validar la instalación de Oracle

. Conéctese a Oracle Server como usuario oracle y ejecute los siguientes comandos:
+
[source, cli]
----
ps -ef | grep ora
----
+

NOTE: Se enumerarán los procesos de oracle si la instalación se ha completado como se esperaba y oracle DB ha iniciado

. Inicie sesión en la base de datos para comprobar los valores de configuración de la base de datos y las PDB creadas con los siguientes conjuntos de comandos.
+
[source, cli]
----
[oracle@localhost ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu May 6 12:52:51 2021
Version 19.8.0.0.0

Copyright (c) 1982, 2019, Oracle.  All rights reserved.

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.8.0.0.0

SQL>

SQL> select name, log_mode from v$database;
NAME      LOG_MODE
--------- ------------
CDB2      ARCHIVELOG

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB2_PDB1                      READ WRITE NO
         4 CDB2_PDB2                      READ WRITE NO
         5 CDB2_PDB3                      READ WRITE NO

col svrname form a30
col dirname form a30
select svrname, dirname, nfsversion from v$dnfs_servers;

SQL> col svrname form a30
SQL> col dirname form a30
SQL> select svrname, dirname, nfsversion from v$dnfs_servers;

SVRNAME                        DIRNAME                        NFSVERSION
------------------------------ ------------------------------ ----------------
172.21.126.200                 /rhelora03_u02                 NFSv3.0
172.21.126.200                 /rhelora03_u03                 NFSv3.0
172.21.126.200                 /rhelora03_u01                 NFSv3.0
----
+
Esto confirma que dNFS funciona correctamente.

. Conéctese a la base de datos a través del listener para comprobar la configuración del listener de Oracle con el siguiente comando. Cambie al puerto de listener y el nombre de servicio de base de datos adecuados.
+
[source, cli]
----
[oracle@localhost ~]$ sqlplus system@//localhost:1523/cdb2_pdb1.cie.netapp.com

SQL*Plus: Release 19.0.0.0.0 - Production on Thu May 6 13:19:57 2021
Version 19.8.0.0.0

Copyright (c) 1982, 2019, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Wed May 05 2021 17:11:11 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.8.0.0.0

SQL> show user
USER is "SYSTEM"
SQL> show con_name
CON_NAME
CDB2_PDB1
----
+
Esto confirma que el listener de Oracle funciona correctamente.





=== ¿Dónde obtener ayuda?

Si necesita ayuda con el kit de herramientas, por favor únase al link:https://netapppub.slack.com/archives/C021R4WC0LC["La comunidad de automatización de soluciones de NetApp admite el canal de Slack"] y busque el canal de automatización de soluciones para publicar sus preguntas o preguntas.
